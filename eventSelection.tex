\chapter{Analysis strategy and event selection} %day1
\label{chap:selection}

Given the theoretical motivation for \BSM physics presented in
Chapter~\ref{chap:theory}, the next Chapters of this thesis describe a
search for \SUSY with the \CMS detector in $13~\tev$ proton collisions
produced by the \LHC. The analysis is designed to target \SUSY
produced through a strong force $pp$-interaction that decays hadronically via
\SM particles to a weakly interacting \LSP. The typical \SUSY models
considered in this search are those with gluino or squark pair
production. However, it is possible to generalise the search to look
for any \BSM signatures that have a weakly interacting particle with
significant momentum in the final state.
% introduce analysis and what the next chapters are about
% hadronic, inclusive, targets gluino and squark production but can be
% generalised to DM

As \SUSY has a significant number of free parameters, it can manifest
itself in a wide range of decay topologies. For this reason the
analysis is aimed to be as inclusive as possible, covering as much
parameter space as is feasible.  This is dictated by an ability to
predict the backgrounds from \SM processes with minimal uncertainties.
The search then attempts to find statistically significant excesses in
the data counts above those predicted for the various backgrounds. To
maximise the chance of seeing \BSM phenomena events are typically
required to have significant hadronic activity and missing momentum.  
% Robust strategy - minimise background uncertainties and look for
% excesses on top, in a conservative way (be very general here)


%%%%%%%%%%%%%%%%%%%%
\section{Challenges for a hadronic \BSM search}
\label{sec:challenge}

%main challenge in limiting and understanding backgrounds
The main challenge for a hadronic \BSM search is in limiting and
understanding the backgrounds, while maximising acceptance of a wide
range of potential \BSM signals. Within the analysis presented in this
Chapter the backgrounds are considered in two separate categories,
either as coming from a \QCD multijet process or from another \SM process
with neutrinos in the final state. As the search can look for generic
signatures that are indicative of \BSM processes, considerations of
signal modelling are not as important as constraining
and understanding these backgrounds.

\subsection{The QCD multijet background}
\label{sec:qcdMultijet}

The most abundant \SM process in $pp$-collisions at the \LHC is \QCD
multijet production through strong force interactions. This background
is orders of magnitude greater than other processes, as is evident
from the total \LHC cross section in Fig.~\ref{fig:xsecs}. The
majority of the \QCD background consists of events with multiple jets,
most commonly two, produced in a balanced configuration, with no
significant missing momentum, \MET or \MHT. A requirement on these
variables therefore significantly reduces the background. As
the abundance of such events is so large, however, mismeasurement of
the jets can lead to a significant \QCD background passing such
requirements. The fake missing energy can be introduced in a variety
of ways:
\begin{itemize}
\item{Detector effects leading to inefficiencies, such as jets
falling into regions of the detector that or uninstrumented or have a
reduced response due to hardware issues.}
\item{Fake additional energy being introduced. This can occur through
detector malfunctions, for example from problems in the readout
electronics. It can also be introduced through \PU that is attributed to
the primary vertex. Additionally, beam interactions that produce
particles outside the detector can introduce fake \MET signatures.}
\item{Issues at the reconstruction stage can also introduce fake \MET,
such as through under or over-corrections of jet energies, or through
the reconstruction algorithms missing physics objects that were
produced.}
\item{Through the measured energy of physics objects falling below
their selection threshold. }
\end{itemize}

Various techniques are applied to remove the background from these
effects, as described throughout this Chapter. Due to the nature of
these effects, which are often time dependent, they cannot be fully
accounted for in simulation. This fact, coupled with the difficulty in
accurately simulating \QCD processes due to uncertainties in the
theoretical calculations, makes constraining the multijet background a
significant challenge. 

It is also possible for the \QCD multijet background to be produced in
association with genuine \MET. This most often occurs in the case of
\emph{heavy flavour} \QCD, where a $b\bar{b}$ quark pair are produced
and at least one the $b$-quarks decay via the weak force, producing a
neutrino.  This is subdominant to the mismeasured multijet background,
but must still be accounted for. 
% heavy flavour qcd

% relevant to the trigger (cite back advantages from trigger chapter)

\subsection{Backgrounds from \SM processes with genuine
\MET}

The other class of \SM backgrounds are those produced with genuine
\MET. These processes only occur in the \SM through the electroweak
production of neutrinos. Due to the
mass scale of such electroweak processes, they commonly occur in
association with jets of significant momenta. The dominant
processes that constitute this background are \wj, \zj and \ttbar,
with residual backgrounds from other, lower cross section,
vector-boson or top production processes.

As a hadronic analysis does not look for leptons in the final state,
vetoing isolated leptons can help to reduce the background from the
\wj and \ttbar processes. In these cases a lepton is always produced
in association with the neutrino that results in the significant \MET.
Such events will only present a problem when the leptons are not
reconstructed, or do not appear to be isolated.  If the W boson
decays via a \tau-lepton that subsequently decays hadronically, the
event will not be removed by the lepton veto.  This is a comparably
small background, however.

The major irreducible background in \BSM searches with \MET comes from 
\znunu processes. They produce a pure missing energy signature with no
associated leptons to veto. Understanding this background to a high
degree of accuracy constitutes a significant challenge. However, due
to the better simulation of electroweak processes compared to \QCD
processes, the simulation can be used to help understand this
background with smaller uncertainties than the \QCD background.
% Z nu nu
%plots with backgrounds: http://www.hep.ph.ic.ac.uk/~mc3909/procs/

%%%%%%%%%%%%%%%%%%%%
\section{The \alphat analysis} 

The search for \SUSY described within this thesis revolves around
suppression of the \QCD multijet background to negligible levels
through the use of topological variables, including \alphat, described
in Sec.~\ref{sec:topoVars}. With negligible \QCD, the large
uncertainties that are usually prevalent when predicting this
background are greatly reduced. The use of these topological variables
also provides robustness against a wide range of mismeasurement
effects, which are particularly relevant when looking at early data,
when the issues present within the detector are not as well
understood. The remaining background from electroweak processes is
then estimated in a \emph{data-driven} way, minimising the
uncertainties from simulation.

During Run~1 of the \LHC, the \alphat analysis was performed
extensively to search for \SUSY. Datasets of $4.98~\ifb$ at
$\sqrt{s}=7~\tev$ and $18.5~\ifb$ at $\sqrt{s}=7~\tev$ have been
analysed, setting limits on \SUSY production within the context of
simplified models
\cite{Chatrchyan:2011zy,Khachatryan:2011tk,Chatrchyan:2012wa,Chatrchyan:2013mys,Khachatryan:2016pxa}.
The analysis presented within this thesis is performed on the data
collected during Run~2 of the \LHC at $\sqrt{s}=13~\tev$. The analysis
has been redesigned and optimised to perform well at the higher centre
of mass energy. Significant improvements have been made to improve the
sensitivity of the analysis and in characterising the various
backgrounds.

%%%%%%%%%%%%%%%%%%%%
\section{QCD multijet suppression with topological variables} %day2
\label{sec:topoVars}

As discussed in Sec.~\ref{sec:qcdMultijet}, a pure requirement on the
\MET of an event is not always enough to remove the \QCD multijet
background.  To effectively remove it, the mismeasurement that leads
to the fake \MET signature must be identified and used to remove
mismeasured events. This can be achieved by making use of topological
variables, which use specific facts about the topology of an event to
decide whether the \MET comes from mismeasurement or a weakly
interacting particle. 
%why topological variables work!! - fake met from detector cock ups

\subsection{The \alphat variable}
%introduce the variables and why they're useful

The dimensionless variable, \alphat, was originally proposed as the
$\alpha$ variable \cite{Randall:2008rw}, but changed to a transverse
quantity to make it more suitable for use in a hadron collider
\cite{CMS-PAS-SUS-08-005,CMS-PAS-SUS-09-001}. It is intrinsically
robust against jet energy mismeasurements in multijet systems. For a
dijet system, \alphat is defined as: 
\begin{equation}
\alpha_T=\frac{E_T^{j_2}}{M_T}, 
\end{equation} 
where $E_T^{j_2}$ is the transverse energy of the lower energy jet and
$M_T$ is the invariant mass of the dijet
system, defined as:
\begin{equation}
M_T=\sqrt{(\Sigma E_T^{j_i})^2-(\Sigma p_x^{j_i})^2-(\Sigma
p_y^{j_i})^2},
\end{equation}
where $E_T^{j_i}$ is the transverse energy of jet, $j_i$ and the $x$
and $y$ components of the transverse momentum are $p_x^{j_i}$ and
$p_y^{j_i}$ respectively.

In the case that the event in question is a perfectly measured dijet
event $E_T^{j_1}=E_T^{j_2}$ and both jets are back-to-back in $\phi$.
This results in a value of $\alphat=0.5$ when the momentum of each jet
is large in comparison with its mass, as is usually the case in
\QCD multijet events. If the jets are still back-to-back but one of
them is mismeasured, this will result in a value of $\alphat<0.5$.
However, in the case that the two jets are recoiling from a genuine
source of \MET, they will not be back-to-back and generally $\alphat>0.5$.

For events with more than two jets, a \emph{pseudo-dijet} system is
formed by summing jets vectorially to combine them.  The system chosen
is one that minimises $|\Delta E_T|$, the difference between the $E_T$
of each pseudo-jet, where $E_T$ is the scalar sum of the transverse
energies of all the jets in each pseudo-jet. This form of clustering
is chosen as it forms the most balanced configuration, making the
pseudo dijet event appear as close to an event with no mismeasurement
as possible. It leads to a generalised form of $\alpha_T$: 
\begin{equation}
\alpha_T=\frac{\Sigma E_T^{j_i}-\Delta
E_T}{2\sqrt{(\Sigma E_T^{j_i})^2-\cancel{H}_T^2}}.
\end{equation} 
In the case that there is no missing energy, $\Delta
E_T=\MHT=0$ and $\alphat=0.5$. However, if the energy of the jets is
mismeasured, the value of $\Delta E_T$ will be very close to \MET,
resulting in $\alphat<0.5$. When the jets are recoiling from genuine
\MET, $\Delta E_T\sim 0$ and $\alphat>0.5$.
  
The efficacy of an $\alphat$ cut in removing the \QCD multijet
background is evident in Fig.~\ref{fig:alphaT}. By
choosing an appropriate cut above $0.5$ it is possible to reduce the
multijet background to a negligible level.

\begin{figure}
	\begin{center}
		\includegraphics[width=0.7\linewidth]{figs/analysis/eventSelection/CMS-PAS-SUS-16-016_Figure-aux_001}%alphaT1_bkgd}
	\end{center}
  \caption{The $\alpha_T$ distribution for events with $H_T>200$ GeV
  that pass the pre-selection criteria (Sec.~\ref{sec:preselection}) when
  $\alphat<0.55$ and the signal selection criteria
  (Sec.~\ref{sec:signalregion}) when
  $\alphat>0.55$. The green dotted line shows the expected multijet
  QCD background that can be removed with an appropriate cut on
  $\alpha_T$}
	\label{fig:alphaT}
\end{figure}

\subsection{The \bdphi variable}

To further suppress the \QCD multijet background after applying an
\alphat cut the biased-$\Delta\phi$, \bdphi, variable is introduced.
This is a variation on the $\Delta\phi$ variable, which is usually
defined as the minimum azimuthal angle between the \MET and any jet in
the event. When calculating \bdphi, each jet is compared to the
\MHT, but the \MHT is recalculated as if the jet were not present in
the event. This leads to the definition:
\begin{equation}
\bdphi = \mathrm{min}(\Delta\phi(\vec{p_T^{j_i}},\vec{\MHT^{j_i}})),
\end{equation}
considered over every jet, $j_i$, and where
$\vec{\MHT^{j_i}}=\vec{\MHT}+\vec{p_T^{j_i}}$. The removal of the
probe jet from the event adds robustness to over-measurement, as well
as under-measurement, of the jet energies.
% introduce and explain

The \bdphi variable acts to remove events in which the \MET is
collinear with the jets, a typical feature of mismeasurement. However,
it also helps to remove the heavy flavour \QCD multijet background,
where a $b\bar{b}$ pair is produced and decays leptonically, producing
neutrinos. These neutrinos are a genuine source of \MET that are
typically boosted along the jet direction. In these cases the \bdphi
helps to reject the \QCD background that may have passed an $\alphat$
cut. The \bdphi distribution for multijet events and the remaining \SM
backgrounds is shown in Fig.~\ref{fig:bdphi}. A requirement on \bdphi
above $\sim 0.5$ will significantly reduce any \QCD multijet background.
%bDPhi augments alphaT when considering mismeasurement in heavy
%flavour qcd stuff
% show the plot from Mark

\begin{figure}
	\begin{center}
		\includegraphics[width=0.7\linewidth]{figs/analysis/eventSelection/CMS-PAS-SUS-16-016_Figure-aux_002}%alphaT1_bkgd}
	\end{center}
  \caption{The \bdphi distribution for events with $H_T>800$ GeV that
  pass the pre-selection criteria (Sec.~\ref{sec:preselection}). The
  green dotted line shows the expected multijet QCD background that
  can be removed with an appropriate cut on \bdphi}
	\label{fig:bdphi}
\end{figure}

%put the stuff about bDPhi over dPhi as in the ICHEP note

% One other variable that is commonly used to perform a similar
% background rejection to \bdphi is the minimum angular separation
% between the missing energy and leading four jets,
% $\Delta\phi(j_{1234},\MHT)_{min}$.  
To further demonstrate the efficacy of the \bdphi variable,
Fig.~\ref{fig:bDPhi_nominal} shows a comparison of the abilities of
the \bdphi variable to control the \QCD multijet background with a
similar jet-\mht angular variable \dphimhtj, the minimum azimuthal
separation between the \mht-vector and the leading four jets. The
\bdphi variable exhibits a distribution that is more sharply peaked
for the \QCD multijet background at low values and faster falling than
\dphimhtj,  \dphimhtj for QCD conversely has a broader distribution
with a larger leakage for a comparable azimuthal selection. This
demonstrates the ability of \bdphi to provide a better control of the
\QCD background while retaining acceptance of events with genuine
\mht, in this case represented by the V+jets, \ttbar and other
residual SM backgrounds with genuine \met. 

\begin{figure}[!h]
 \centering
 \subfloat[\bdphi distribution.\label{fig:bDPhi_dist}]{
 \includegraphics[width=0.5\textwidth]{figs/analysis/eventSelection/biasedDPhi_all_800}
 }
 \subfloat[\dphimhtj distribution.\label{fig:DPhiMht_dist}]{
 \includegraphics[width=0.5\textwidth]{figs/analysis/eventSelection/minDeltaPhiMht_all_800}
 } \\
 \caption{\bdphi and \dphimhtj distributions of MC simulation of the
 dominant analysis backgrounds
 after analysis selections for \scalht $> 800$ \GeV. }
 \label{fig:bDPhi_nominal}
\end{figure}

This is further demonstrated in Fig.~\ref{fig:bDPhi_roc}, where the
efficiency of retaining processes with genuine \mht is plotted against
the background efficiency for a series of \bdphi, \dphimhtj and
\dphimhtjall cuts, where \dphimhtjall considers all jets rather than
just the leading four. The points with a cut of $0.5$ are highlighted
as stars on the plot. As the general analysis strategy involves reducing
the \QCD multijet background to a negligible level while maximising
signal acceptance, these plots demonstrate that this is most
achievable with a cut on \bdphi. For the same threshold requirement of
$0.5$ on each variable, the \bdphi variable provides an efficiency for
multijet events that is approximately three orders of magnitude lower
than for \dphimhtj at the cost of approximately a factor $3$ reduction
in signal efficiency. The threshold on \bdphi required to give an
approximately equivalent suppression of QCD achieved with the
\dphimhtj variable is larger than 1.5, at the cost of a loss of a
factor $5$ in signal acceptance w.r.t. \bdphi. This also holds true in
the extreme case of a high jet multiplicity signal model.
Additionally, despite performing similarly to \bdphi for separating
the non-multijet from multijet backgrounds, the \dphimhtjall variable
performs worse for a high jet multiplicity signal model than \bdphi.

\begin{figure}[!h]
 \centering
 \subfloat[Acceptance of SM backgrounds with genuine \met vs QCD
 acceptance]{
 \includegraphics[width=0.5\textwidth]{figs/analysis/eventSelection/rateEffEwkQCD}
 }~
 \subfloat[High jet multiplicity \SUSY model acceptance vs QCD acceptance]{
 \includegraphics[width=0.5\textwidth]{figs/analysis/eventSelection/rateEffSignalQCD}
 } \\
 \caption{\bdphi, \dphimhtj and \dphimhtjall efficiency for simulation of processes with genuine
 \met vs QCD multijet background efficiency. The stars correspond to
 efficiencies with a cut of $0.5$ on each variable. A generic case of
 non-multijet process efficiency is considered in (a). In (b) we
 consider an uncompressed \SUSY model where gluinos are produced and
 decay via four tops to a pair of \acp{LSP}. 
%  This is the case in which \bdphi is
%  expected to have the lowest efficiency due to the very high jet
%  multiplicity of the model.
 }
 \label{fig:bDPhi_roc}
\end{figure}

Additionally the \bdphi variable displays robustness in the presence
of severe event mismeasurement, which is not present in \dphimhtj. A
mismeasurement is simulated by artificially lowering the \pt of the
jet that minimises the azimuthal separation variable to 41~\gev. Due
to the removal of the probe jet from the computation of \bdphi, the
distribution of angular separation Fig.~\ref{fig:shifted_bDPhi_dist}
is remains unchanged under severe mismeasurement. The \dphimhtj
variable is sensitive to such mismeasurement as both \mht and the rank
of the leading four jets are affected, resulting in a broader
distribution with increased leakage
Fig.~\ref{fig:shifted_DPhiMht_dist}.

\begin{figure}[!h]
 \centering
 \subfloat[QCD \bdphi distribution with mismeasurement.\label{fig:shifted_bDPhi_dist}]{
 \includegraphics[width=0.5\textwidth]{figs/analysis/eventSelection/shiftedMinBDPhi_all_800}
 }
 \subfloat[QCD \dphimhtj distribution with mismeasurement.\label{fig:shifted_DPhiMht_dist}]{
 \includegraphics[width=0.5\textwidth]{figs/analysis/eventSelection/shiftedMinDeltaPhiMht_all_800}
 } \\
 \caption{\bdphi and \dphimhtj distributions of QCD multijet simulation
 after analysis selections for \scalht $> 800$ \GeV in the case of
 severe mismeasurement. The
 total number of events that pass a $\Delta\phi > 0.5$ selection of the
 respective quantity are indicated. }
 \label{fig:bDPhi_mismeasured}
\end{figure}

\subsection{The missing energy ratio \mhtmet}
\label{sec:mhtmet}

After requirements are made on the \alphat and \bdphi variables, the
majority of events with \MHT introduced from jet mismeasurement are
removed. However, this does not take account of events in which jets
fall below their reconstruction threshold, detailed for this analysis
in Sec.~\ref{sec:physobj}. If several jets fall just below threshold
they can fake a significant \MHT. There is a much lower threshold
on the particles that go into the \MET calculation than the jets that
go into the \MHT threshold. Therefore, a requirement on the ratio of
these two variables helps to remove the background from mismeasurement
that falls below threshold. The extent to which this can remove the
remaining \QCD multijet backgroudn after the \alphat and \bdphi cuts
is shown in Fig.~\ref{fig:mhtDivMet}.
% removes soft stuff! given jet threshold

\begin{figure}
	\begin{center}
		\includegraphics[width=0.5\linewidth]{figs/analysis/eventSelection/mht40_pt_Div_metNoX_pt_all_all_80X_noOverflow_scaled0p6}%alphaT1_bkgd}
	\end{center}
  \caption{The \mhtmet distribution for \MC simulation with
  2.3~\ifb of $\sqrt{s}=13~\tev$ data overlaid. The
  normalisation of the \MC simulation is scaled so the data and \MC
  match.}
	\label{fig:mhtDivMet}
\end{figure}

%%%%%%%%%%%%%%%%%%%%
\section{Physics objects} %day3
\label{sec:physobj}

The analysis makes use of physics objects that are reconstructed with
the algorithms described in Chapter~\ref{chap:reconstruction}. As
there are many different input parameters within each of the
algorithms, the specific details relevant to the analysis are
described in this section. Jets are reconstructed to characterise the
hadronic activity within the event. Muons, electrons and photons are
reconstructed for a signal region veto and for selecting the \emph{control
regions} that are used for background estimation, detailed in
Sec.~\ref{sec:controlregions}. \emph{Isolated tracks} are additionally
reconstructed to provide an extra type of lepton veto when the leptons
are not fully reconstructed.

\subsection{Jets}
\label{sec:evSel_jets}

Jets are defined as sets of \PF candidates clustered by
the anti-$k_{T}$ jet clustering algorithm with
a distance parameter of 0.4. Additionally, \ac{CHS} is applied, charged
hadrons that can be traced back to \PU vertices are not clustered. The
four-vectors of the jets are then corrected with the procedure
outlined in Sec.~\ref{sec:reco_jec}.

The \emph{loose} working point jet quality criteria is chosen, defined
by the selections listed in Tab.~\ref{tab:loose-jet-id}. All jets are
required to contain at least part of their energy within the \ECAL and
\HCAL and must consist of at least one of each different type of
particle. This is effective at removing fake jets that are the result
of detector noise.  
%maybe show plots here?

Jets are also $b$-tagged with the \emph{medium} working point of the
algorithm described in Sec.~\ref{sec:reco_btag}. This is obtained with
a cut of $>$ 0.800 on the algorithm discriminator variable. This
results in a gluon/light-quark mis-tag rate of $\sim$1 \% (where
\emph{light} means $u$, $d$ and $s$ quarks), a charm-quark mis-tag
rate of $\sim$10 \% and a b-quark b-tag efficiency of about 60 \%. 

\begin{table}[ht!]
  \caption{The \emph{loose} jet ID requirements. \label{tab:loose-jet-id}}
  \centering
  \begin{tabular}{ ccc }
    \hline
    \hline
    Variable & cut & notes \\ \hline
    \multicolumn{3}{c}{$-3.0 < \eta_{\mathrm{jet}} < 3.0$} \\ \hline    
    Neutral Hadron Fraction & $<0.99$ & - \\
    Neutral Electromagnetic Fraction & $<0.99$ & - \\
    Number of constituents & $>1$ & - \\
    Charged Hadron Fraction & $>0$ & only for $|\eta_{\mathrm{jet}}| < 2.4$ \\
    Charged Multiplicity & $>0$ & only for $|\eta_{\mathrm{jet}}| < 2.4$ \\
    Charged Electromagnetic Fraction & $<0.99$ & only for $|\eta_{\mathrm{jet}}| < 2.4$ \\ \hline
    \multicolumn{3}{c}{$|\eta_{\mathrm{jet}}| > 3.0$} \\ \hline        
    Neutral Electromagnetic Fraction & $<0.90$ & - \\
    Number of Neutral Particles & $>10$ & - \\
    \hline
    \hline
  \end{tabular}
\end{table}

\subsection{Muons}
\label{sec:muon-id}
Two types of selection are made on muons depending on whether they are
used to carry out the veto in the signal region or used for selecting
one of the control regions. A tighter criteria is used for the control
region, muons are required to be well isolated and be
reconstructed with the global algorithm, as detailed in
Sec.~\ref{sec:muons_reco}. The muon must consist of a track
with at least 5 inner layer hits, a good fit $\chi^2$, good compatibility with
the primary vertex and significant muon chamber hits. The isolation in
the control region uses the relative isolation algorithm
defined in Sec.~\ref{sec:reco_iso} with effective-area
correction, it is required that $I^{\textrm{rel}}<0.15$.  

For the purpose of vetoing muons in the signal region, a looser
working point is used, which provides $\sim98\%$ efficiency. The
muon is just required to be classified within the \PF algorithm and
reconstructed with the global or tracker muon
algorithms. The
mini-isolation algorithm is utilised with effective-area \PU
correction $I^{\textrm{rel}}_{\textrm{mini}} < 0.2$. The reconstructed
muon must have a $\pT>10~\gev$ and be within $|\eta|<2.1$.

\subsection{Photons}
\label{sec:photon-id}

Photons are identified according to a \emph{tight} working point
definition ($\sim$ 71 $\%$ efficiency) and required to be well
isolated.  A \PF-based isolation which considers neutral and charged
hadrons separately is used with a cone size $\Delta R<0.3$
and effective-area corrections are applied to remove the
effects of pileup.  Table \ref{tab:photon-id-gamma}
summarises the isolation selection used. There are additional
requirements on the ratio of \HCAL and \ECAL deposits and the
kinematics of the deposits within the \ECAL. The reconstructed
photon must have a $\pT>25~\gev$ and be within $|\eta|<2.5$.

\begin{table}[ht!]
  \caption{Photon isolation criteria (\emph{tight} working point). The
  energy of particles within the isolation cone must be less than the
  value in the column.\label{tab:photon-id-gamma}}
  \centering
  \footnotesize
  \begin{tabular}{ ccc }
    \hline
    \hline
    Categories                    & Barrel                             & EndCap                             \\
    \hline
    PF charged hadron isolation   & 1.66 GeV & 1.04 GeV                          \\
    PF neutral hadron isolation   & 0.14 + $ e^{0.0028 \times \pt^{\gamma} + 0.5408}$  &  3.89 + 0.0172 $\times$ $\pt^{\gamma}$\\
    PF photon isolation           & 1.40 + 0.0014 $\times$ $\pt^{\gamma}$ & 1.40 + 0.0091 $\times$ $\pt^{\gamma}$ \\
    \hline
    \hline
  \end{tabular}
  \end{table}

\subsection{Electrons}
\label{sec:electron-id}

In order to veto electrons a \emph{loose} working point definition
($\sim$ 90 $\%$ efficiency) is used. There are requirements on the
ratio of \HCAL and \ECAL deposits, criteria to remove photons that
have converted within the tracker, a minimum number of tracks required
and kinematic requirements on the deposit within the \ECAL. Electrons
are also require required to be isolated with the effective-area
corrected mini-isolation algorithm.  Isolated electrons are defined by
$I^{\textrm{rel}}_{\textrm{mini}} < 0.1$. The reconstructed electron
must have a $\pT>10~\gev$ and be within $|\eta|<2.5$.

% Table \ref{tab:ele-id} summarises the identification 
% selection used. 
%
% \begin{table}[h!]
%   \caption{Electron identification (``tight'' working point).\label{tab:ele-id}}
%   \centering
%   \footnotesize
%   \begin{tabular}{ lcc }
%     \hline
%     \hline
%     Categories                                               & Barrel    & EndCap    \\
%     \hline
%     $\Delta \eta_{In}$                                       & 0.0105   & 0.00814  \\
%     $\Delta \phi_{In}$                                       & 0.115    & 0.182  \\
%     $\sigma_{i\eta i\eta}$                                   & 0.0103    & 0.0301  \\
%     H/E                                                      & 0.104    & 0.0897   \\
%     d0 (vtx)                                                 & 0.0261    & 0.118  \\
%     dZ (vtx)                                                 & 0.041    & 0.822  \\
%     $\lvert(1/E_{\textrm{ECAL}} - 1/p_{\textrm{trk}})\rvert$ & 0.102     & 0.126  \\
%     Missing hits (inner tracker)                             & 2         & 1         \\
%     Conversion veto                                          & yes       & yes   \\
%     \hline
%     \hline
%   \end{tabular}
%   \end{table}


\subsection{Isolated tracks}
\label{sec:SIT}

A single isolated track comprises a charged PF candidate with $\Pt >
10 \gev$, $\Delta z(\mathrm{track}, \mathrm{\ac{PV}}) < 0.05 \,
\mathrm{cm}$ and with a relative isolation smaller than 0.1, where the
isolation is determined from the sum of the \Pt of the charged PF
candidates within $\Delta R < 0.3$. It is useful for identifying W
bosons through their leptonic decays, when the lepton is not fully
reconstructed: W $\rightarrow$ $\mu \nu$, W $\rightarrow$ $e\nu$, and
W $\rightarrow$ $\tau$($\rightarrow l$) $\nu$.  Single prong decays of
the tau lepton can also be identified: W $\rightarrow$ $\tau$
($\rightarrow$ h$^{\pm}$ + n$\pi^{0}$) $\nu$. 
%Events in the signal region and all control regions (ignoring the
%selected muons) containing a SIT candidate identified with these
%criteria are vetoed.

\subsection{Energy sums}

Missing transverse momentum (\MET) is reconstructed as described in
Sec.~\ref{sec:met_reco} and the the Type-I \MET energy correction is applied.
The \met is used in the definition of 
the transverse mass, $M_{T}$, which is in turn used as part of
the selection criteria that define the single muon control sample 
(Sec.~\ref{sec:controlregions}), and for the $\mhtmet$ cleaning
filter, described in Sec.~\ref{sec:mhtmet}.

The \HT and \MHT energy sums are constructed with the jets outlined in
Sec.~\ref{sec:evSel_jets}, which are subject to the kinematic requirements of
$\pT>40$~\gev and $|\eta|<3$.

%%%%%%%%%%%%%%%%%%%%
\section{Trigger strategy}
\label{sec:trigStrat}

As \SUSY models have a lot of freedom in how they are manifest, a
guiding principle of the analysis is to maintain acceptance of as much
phase space as possible. Despite some \SUSY models being ruled out at
energies $O(100~\gev)$, models with compressed spectra are still poorly
constrained in this regime. The trigger strategy therefore revolves
around maintaining as low an energy threshold as possible given the
very high rate of \QCD multijet processes. To select for events with
significant hadronic activity and missing energy, triggers are used
that rely on the \HT and \MHT of the events. Additionally, including
topological variables, such as \alphat, in the \HLT allow events to be
collected with $\HT$ as low as $200~\gev$.

To collect events that are of general use to hadronic analyses, the
$\HT$ of each event is reconstructed within the \HLT. This is carried
out with a custom form of \PF, that is designed to work in a way that
is quick enough given the time the trigger has to make a decision. To
further help with this timing constraint, the \HT is calculated with
jets clustered from only calorimeter deposits and a looser constraint
is made, reducing the load on the \PF reconstruction. This allows for
all events that pass $\HT>800~\gev$, calculated by the \HLT,
to be collected. When carrying out full online reconstruction, this
results in a $\sim 100\%$ efficiency for events with an offline
$\HT>900~\gev$. 

To be able to efficiently collect events with lower
values of \HT, the \alphat variable is calculated within the \HLT. A
requirement is made on both \alphat and \HT for a series of different
\HT thresholds. These requirements are chosen to limit the rate of the
\alphat-\HT triggers to an acceptable level, while maintaining low
acceptance of events with low \HT. 

There are also general purpose missing energy triggers that require
both an \MHT and \MET threshold to be surpassed within the \HLT.
Including events selected by these triggers helps to increase the
efficiency across the phase space. These variables, however, are quite sensitive to
running conditions and mismeasurement, so are not as robust against
the \HLT rate as the \alphat-\HT triggers. The full list of
triggers used to collect data for the signal region of the \alphat
analysis can be seen in Tab.~\ref{tab:triggers}.

The events in the control regions are collected with muon or photon
triggers, where the \HLT requires at least one of these objects above
a certain energy threshold. Due to the lower rate of events
containing leptons, these triggers are at full efficiency for their
requirements in the analysis. The specific triggers used are also listed in
Tab.~\ref{tab:triggers}.

\begin{table}[h!]
\caption{Trigger thresholds of the Level-1 and \HLT paths for the
hadronic signal region and the leptonic control regions. {\bf FIXME} check the CR l1 thresholds }
\footnotesize
\centering
\begin{tabular}{c|c|c} 
Analysis region   & Level-1 requirements & \HLT requirements                                                \\
\hline
 & $\HT>240~\gev~\textrm{or}~\MET>70~\gev$ & $\HT>200~\gev,~\alphat>0.57,\MET>90~\gev$ \\
 & $\HT>240~\gev~\textrm{or}~\MET>70~\gev$ & $\HT>250~\gev,~\alphat>0.55,\MET>90~\gev$ \\
 & $\HT>240~\gev~\textrm{or}~\MET>70~\gev$ & $\HT>300~\gev,~\alphat>0.53,\MET>90~\gev$ \\
Signal & $\HT>240~\gev~\textrm{or}~\MET>70~\gev$ & $\HT>350~\gev,~\alphat>0.52,\MET>90~\gev$ \\
 & $\HT>240~\gev~\textrm{or}~\MET>70~\gev$ & $\HT>400~\gev,~\alphat>0.51,\MET>90~\gev$ \\
 & $\HT>240~\gev$ & $\HT>800~\gev$ \\
 & $\MET>70~\gev$ & $\MET>90~\gev~\textrm{or}~\MHT>90~\gev$ \\
\hline
\mj& $\pT^{\mu}>10~\gev$ & $\pT^{\mu}>22~\gev$ \\
\mmj& $\pT^{\mu}>10~\gev$ & $\pT^{\mu}>22~\gev$ \\
\gj& $\HT>240~\gev~\textrm{or}~\pT^{\gamma}>25~\gev$ & $\HT>800~\gev~\textrm{or}~\pT^{\gamma}>175~\gev$ \\

\end{tabular}
\label{tab:triggers}
\end{table}

%%%%%%%%%%%%%%%%%%%%
\section{Pre-selection}%day4
\label{sec:preselection}

To ensure that all events considered by the analysis have
significant hadronic activity and missing energy, a pre-selection is
carried out on all events considered in the signal and control
regions. It is described within this section.

Events are required to contain at least one $p_T>100$~GeV jet,
with all other jets being considered if they have $p_T>40$~GeV and are
well reconstructed in the central region, $|\eta|<3$. If any jets
fall outside the $\eta$ range, the event is vetoed. This ensures there
is no significant energy deposited within the region of the detector
with no tracker. As most \BSM physics scenarios result in the
production of particles at a high mass scale, they are expected to deposit
most of their energy centrally. Therefore, this \emph{forward jet
veto} does not significantly effect the signal efficiency for most
models. The \QCD multijet background consists of many
soft scattering events, which typically deposit a significant
proportion of energy within the forward region. As this is the case,
the forward jet veto also helps to reduce the multijet
background.

Significant hadronic activity is selected by requiring $H_T>130$~GeV.
This cut is primarily motivated by the trigger threshold, it is the
lowest \HT value that can be reached with a reasonable trigger rate
and efficiency. To ensure there is significant missing energy, it is
required that $\MHT>200~\gev$.  This threshold is chosen as roughly
equivalent to the magnitude of the \MHT requirement made by the
\alphat cuts on dijet events within the signal region when there is no
significant mismeasurement. 
%could expand on this with formula?

Additionally, to remove events with significant energy deposits that
have not been reconstructed as jets, a requirement is made on the
ratio $\mhtmet<1.25$, as motivated in Sec.~\ref{sec:mhtmet}.  When
this requirement is made for the control regions, the \MET is
reconstructed ignoring the leptons or photons that define the
particular region. This ensures a fair comparison with \MHT and allows
the lepton to simulate the missing momentum in the 
signal region.

To ensure that beam conditions or reconstruction effects have
not produced a spurious \MET, extra filters are applied. The filters
used are designed to remove instances of fake \MET while retaining
a high efficiency for real physics events, they are described in the
following \cite{1748-0221-10-02-P02006}.
\begin{itemize}
\item{The primary vertex must be well reconstructed, within
$|z|<=24$~cm of the proton collision region and $d_{xy}<2$~cm from the
beam line}
\item{Beam interactions exterior to the detector, \emph{beam halo} effects,
are removed by vetoing events with \ac{CSC} and calorimeter energy
deposits consistent with interactions from particles outside the
detector}
\item{Noise, caused by particle interactions with the read-out system
and dead regions within the \HCAL, are removed with the \emph{HBHE
noise and isolation filters}}
\item{Anomalous signals within the \ECAL endcap supercrystals are removed
with the \emph{\ECAL Endcap SC Noise filter} and \emph{\ECAL \ac{TP}
filter}}
\item{Events with misidentified straight tracks that are reconstructed
to have a large \pT are removed by the \emph{bad track filter}}
\end{itemize}

Finally, two additional vetoes are applied to ensure events are purely
hadronic with minimal misreconstruction. Any events that contain a
\ac{SIT} with $\pT > 10\gev$ and within $|\eta| < 2.5$ is vetoed. This
reduces the background from single pronged $\tau$ decays and
misreconstructed leptons. Within the muon control regions this track
is ignored if it is within $\Delta R <0.02$ of an identified lepton.
Additionally, any events with jets that do not pass the loose
requirements described in Sec.~\ref{sec:evSel_jets} are vetoed.

A summary of the pre-selection requirements is shown in
Tab.~\ref{tab:preselection}.

\begin{table}[h!]
  \caption{Summary of the pre-selection criteria.}
  \centering
  \footnotesize
  \begin{tabular}{ ll }
    \hline
    Selection                     & Requirement                     \\
    \hline
    ``MET filters''               & \parbox[t]{10cm}{Primary Vertex, CSC Beam Halo,
      HBHE Noise and Isolation, \\ ECAL Endcap SC Noise, ECAL TP, bad
      track filter}         \\
    Jet acceptance                & $\pT > 40\gev$, $|\eta| < 2.4$                      \\
    Lead jet acceptance           & $\pT > 100\gev$, $|\eta| < 2.4$                   \\
    Forward jet veto              & $\pT > 40\gev$, $|\eta| > 2.4$                     \\
    \HT requirement               & $\HT > 200\gev$                  \\
    \mht requirement              & $>200\gev$         \\  
    \mhtmet requirement              & $<1.25$         \\  
    Single isolated track veto      & $\pT > 10\gev$, $|\eta| < 2.5$    \\  
    \hline
  \end{tabular}
  \label{tab:preselection}
\end{table}

\section{The signal region}
\label{sec:signalregion}

The signal region selection is chosen to be as inclusive to potential
\BSM signatures in the jets and missing momentum final state, while
minimising the effects of mismeasurement and contributions from \SM
processes with genuine \MET. To reduce the background from decays via
W bosons, a veto is made on any leptons that are identified as
described in Sec.~\ref{sec:physobj}. To maintain a fully hadronic
final state any events containing reconstructed photons are also
vetoed.

After the pre-selection requirements, there is still a significant
multijet background in the signal region. This is reduced to a
negligible level with a \HT dependent cut on the \alphat of the event.
Along with this, a cut is made on the \bdphi variable to ensure any
multijet background that passes the \alphat cut is removed. A summary
of these cuts can be seen in Tab.~\ref{tab:atCut}. The \alphat cuts
are loosened as a function of \HT in correspondence with the trigger
thresholds. At lower values of \HT the high trigger rate requires
a higher \alphat threshold, due to the fact that the multijet
background falls off as a function of \HT. In the case that events
have just one identified jet, the value of \alphat is undefined. In
these \emph{monojet} events there is no \alphat cut. The \bdphi
variable is also undefined with 40~\gev jets. To get around this issue
and provide multijet discrimination in monojet events, the \pT
threshold on jets used to calculate the \bdphi variable is reduced to
25~\gev. In the case that there are no other jets between 25 and
40~\gev the event is allowed into the signal region.

It is observed that the beam halo \MET filter, described in
Sec.~\ref{sec:preselection}, is ineffective in removing all cases of
mismeasurement from the signal region. A requirement that the \ac{CHF}
of the lead jet is above $10\%$ is enough to negate this effect. This
cut is demonstrated to in Fig.~\ref{fig:leadJetCleaning}. Due to the
fact that the proton beam is steered within the horizontal plane of
the \LHC, beam halo effects manifest themselves at \phi values of $0$
and $\pi$. The application of the \ac{CHF} cut removes the spikes in
the leading jet $\phi$ distribution at these values.

\begin{table}[h!]
  \centering
  \footnotesize
  \begin{tabular}{ l|ccccccccc }
    \hline
    \scalht (GeV)      & 200       & 250       & 300       & 350       & 400       & 500       & 600 &  $>$900    \\
    \hline                                                                                     
    \alphat threshold  & 0.65      & 0.60      & 0.55      & 0.53      & 0.52      & 0.52      & 0.52 & --    \\
    \hline
    \bdphi threshold  & 0.5      & 0.5      & 0.5      & 0.5      &
    0.5      & 0.5      & 0.5 & 0.5    \\
    \hline
  \end{tabular}
  \caption{The \alphat and \bdphi thresholds versus
    lower bound of \scalht bin. For all \HT bins satisfying $\HT >
    900\gev$, no \alphat cut is applied. No \alphat requirement is
    imposed in the case that there is only one reconstructed jet.}
  \label{tab:atCut}
\end{table}

% \begin{figure}[h!]
%   \centering
%     %{\includegraphics[width=0.32\textwidth]{figures/selection/jet_chHEF_mono_all_before.pdf}}
%     {\includegraphics[width=0.32\textwidth]{figures/selection/jet_phi[0]_mono_all_before.pdf}}
%     {\includegraphics[width=0.32\textwidth]{figures/selection/jet_phi[0]_mono_all_after.pdf}}
%     \caption{Distributions in the signal region of the jet charged hadron
%     energy fraction (CHF) (Left), jet $\phi$ direction (Centre), and jet $\phi$
%     direction after applying a requirement of {CHF~$>0.1$}. The large excess in data
%     at charged hadron fractions close to zero and ${\phi = 0, \pi}$ is consistent with beam
%     halo effects, and is effectively suppressed by the aforementioned selection.}
%     \label{fig:leadJetCleaning}
% \end{figure}
\begin{figure}[!h]
 \centering
 \subfloat[No \ac{CHF} cut]{
 \includegraphics[width=0.5\textwidth]{figs/analysis/eventSelection/jet_phi[0]_mono_all_before}
 }
 \subfloat[Lead jet \ac{CHF}>0.1]{
 \includegraphics[width=0.5\textwidth]{figs/analysis/eventSelection/jet_phi[0]_mono_all_after}
 } \\
  \caption{Distributions in the signal region of the jet $\phi$
  direction (a), and jet $\phi$ direction after applying a requirement
  of \ac{CHF}~$>0.1$. The large excess in data at charged hadron
  fractions close to zero and ${\phi = 0, \pi}$ is consistent with
  beam halo effects, and is effectively suppressed by the
  aforementioned selection.}
  \label{fig:leadJetCleaning}
\end{figure}

\section{The control regions}
\label{sec:controlregions}

To carry out a data-driven estimation of the \SM backgrounds, several
control regions are defined. They are chosen to be orthogonal to the
signal region by inverting at least one of the requirements. To estimate
processes with genuine \MET, control regions are populated with events
that contain at least one muon or photon. The muons or photons are then
ignored when calculating any of the event level variables, such as
\alphat or \bdphi, to simulate the signature left by neutrinos or
cases in which the lepton is not properly reconstructed. Extrapolation
from these control regions with guidance from simulation is then used
to estimate the background counts in the signal region, as described
in Chapter~\ref{chap:backgroundPred}. The pre-selection criteria
is applied to each control region and ensures they are in a similar
phase-space to the signal region. As the requirement of an isolated
lepton or photon already selects electroweak processes and rejects
\QCD multijet processes there is no need for an \alphat or \bdphi
requirement in these control regions. Removing these requirements
significantly increases the number of events available within each
control region. This reduces statistical uncertainties when they are
used for background estimation.

\subsection{The \mj control region}

The selection criteria for the \mj control region are chosen to select
events containing W bosons that are produced in association with
jets and decay to a muon and a neutrino. Exactly one muon that passes
the control region selection criteria, described in Sec.~\ref{sec:muon-id}, with
$\pT>30~\gev$ and $|\eta|<2.1$ is required. The muon is required to be
separated from other jets by $\Delta R(\mu,\mathrm{jet})>0.5$. If any
other leptons or photons are present in the event then it is vetoed, as
in the signal region.

To help ensure events containing W bosons are selected a requirement
is also made on the transverse mass of the W candidate. It is
approximated by taking the transverse mass between the \MET and the
muon, and is then required to be compatible with the muon mass,
$30<M_T(\mu,\MET)<125~\gev$. This effectively suppresses any residual
\QCD multijet background that pass the muon requirement.
%plots??

\subsection{The \mmj control region}

The selection criteria for the \mmj control region are chosen to
select events containing Z bosons that are produced in association
with jets and decay to a pair of muons. They are selected to be
kinematically similar to the main irreducible background in the signal
region, \znunu. Exactly two muons are required that
pass tight selection criteria with $\pT>30~\gev$ and $|\eta|<2.1$.
They are also required to be separated from jets as in the \mj control
region. Again, if any other leptons or photons are present the event
is vetoed.

To maximise the selection of events containing Z bosons the
invariant mass of the two muons is required to be compatible with the
Z mass, $m_Z$. Events are used if they are within a 25~\gev window,
$m_Z-25<M_{\mu_1\mu_2}<m_Z+25~\gev$.

\subsection{The \gj control region}

The \gj control region is defined to provide a sample of photon events
with significant hadronic energy. High energy photons have similar
kinematic properties to $Z$ bosons, which allows this control region
to augment the prediction of the \znunu background. To
ensure the mass of the Z boson has a negligible effect when making
this assumption, a single photon is required with $\pT>200~\gev$ and
$|\eta|<1.45$. This selection also maintains a high trigger
efficiency, the photon trigger rate is significantly greater than the
muon rate so has a higher \pT threshold. To ensure the photon did not
originate from a hadronic jet, events with $\Delta
R(\mathrm{jet},\gamma)<0.5$ are vetoed. Events containing any photons
or lepton other than the single selected photon are vetoed.

\subsection{The hadronic control regions}

While the above control regions are useful for estimating the \SM
backgrounds with genuine \MET, some hadronic control regions are
defined to quantify the residual \QCD multijet background that remains
in the signal region. To define these control regions, all the signal
region requirements are made except for the requirement on \bdphi or
\mhtmet being inverted. The use of these control regions is
discussed in Sec.~\ref{sec:qcdEstimation}.

\section{Event categorisation}
\label{sec:categorisation}

To help to separate signal from background for a wide range of
different signal hypotheses, events are a categorised based on the
number of jets, \nj, their total hadronic energy, \HT, the magnitude
of the missing hadronic energy, \MHT, and the number of jets tagged as
a $b$-quark, \nb. These particular variables are chosen because many
\BSM models predict excesses in the tails of the distributions that
are not predicted by the \SM. For example, many \SUSY models favour
decays via top quarks, resulting in a significant number of $b$-tagged
jets and high \nj multiplicities. All \BSM models targeted by this
search also produce significant \MHT.

Along with straightforward binning in the categorisation variables,
events are also split based on their jet topology. They are
categorised as either \emph{symmetric}, \emph{asymmetric} or
\emph{monojet} depending on the \pT of the second leading
jet. In the case that the second leading jet has $\pT>100~\gev$ the
event is defined as symmetric. This category of events targets the
typical \SUSY pair production scenarios when there is a significant
mass splitting between the \SUSY parent and the \LSP. However, in the
case that there is a compressed spectrum, a small mass splitting,
there is unlikely to be significant energy in the \SM decay products
of the \SUSY system. Sensitivity to these events can be gained by
selecting events with significant \ac{ISR} or \ac{FSR} that have
recoiled from the invisible \SUSY system. This topology is targeted by
the monojet and asymmetric jet category which require the second jet
to have $\pT<40~\gev$ or $40<\pT<100~\gev$ respectively.
%{\bf FIXME} add something that differentiates the different categories?

A summary of the event categorisation for the \nb, \njet and \HT
variables is shown in Tab.~\ref{tab:eventCategorisation}. This is
mirrored in the control and signal regions. The \MHT dimension is
treated slightly differently, with a variable binning based on \HT.
This will be discussed further in Sec.~\ref{sec:mhtDim}.

\begin{table}[h!]
  \caption{Summary of the \nj, \nb, \HT binning.}
  \label{tab:eventCategorisation}
  \centering
  \begin{tabular}{ cl }
    \hline
    Variable & Binning \\
    \hline
    \multirow{3}{*}{\njet}
     & Monojet:    $\njet=1$ \\
     & Asymmetric: $\njet \geq 2$ \\
     & Symmetric:  $\njet =2,3,4,5 \geq 6$ \\
    \hline
    \nb & $\nb=0,1,2,3,\geq 4$ \\
    \hline
    \scalht (GeV) & 200-400, 400-600, 600-900, 900-1200, $>$1200 \\
    \hline
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%
%\section{Further optimisation of event selection}
% something about minChi here?
